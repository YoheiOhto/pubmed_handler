{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d478fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import glob\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import os \n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2088360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Reading data from SQLite: /workspace/0-utils/1-data/pubmed/db/pubmed_n25_0000.db\n",
      "   -> Data loaded. Total records: 100000\n",
      "\n",
      "2. Connecting to Elasticsearch and creating index: pubmed\n",
      "   -> Index 'pubmed' created successfully with optimized mapping.\n",
      "\n",
      "3. Starting high-speed Bulk Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1400/3035363757.py:83: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  successes, errors = helpers.bulk(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bulk Indexing Complete.\n",
      "   -> Total documents indexed: 100000\n",
      "   -> Successfully indexed: 100000\n",
      "\n",
      "4. Final Index Count Check:\n",
      "   -> Total documents in ES index 'pubmed': 90000\n"
     ]
    }
   ],
   "source": [
    "ES_HOST = \"http://elasticsearch:9200\" \n",
    "ES_PASSWORD = \"\" \n",
    "DB_PATH = \"/workspace/0-utils/1-data/pubmed/db/pubmed_n25_0000.db\" \n",
    "INDEX_NAME = \"pubmed\"\n",
    "BULK_SIZE = 5000\n",
    "\n",
    "print(f\"1. Reading data from SQLite: {DB_PATH}\")\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM raw\", conn)\n",
    "    conn.close()\n",
    "    df = df.fillna(\"\")\n",
    "    print(f\"   -> Data loaded. Total records: {len(df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   -> Error reading SQLite: {e}\")\n",
    "    exit()\n",
    "\n",
    "es = Elasticsearch(\n",
    "    ES_HOST, \n",
    "    basic_auth=(\"elastic\", ES_PASSWORD),\n",
    "    request_timeout=60\n",
    ")\n",
    "\n",
    "print(f\"\\n2. Connecting to Elasticsearch and creating index: {INDEX_NAME}\")\n",
    "\n",
    "MAPPING_BODY = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"english_exact\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"PMID\":        { \"type\": \"keyword\" },\n",
    "      \"TITLE\":       { \"type\": \"text\", \"analyzer\": \"english_exact\" },\n",
    "      \"ABST\":        { \"type\": \"text\", \"analyzer\": \"english_exact\" },  \n",
    "      \"ABST_ENG\":    { \"type\": \"integer\" },\n",
    "      \"TRUNCUTATED\": { \"type\": \"integer\" },\n",
    "      \"JOURNAL\":     { \"type\": \"keyword\" },\n",
    "      \"ISSN\":        { \"type\": \"keyword\" },\n",
    "      \"PUB_YEAR\":    { \"type\": \"integer\" },\n",
    "      \"PUB_MONTH\":   { \"type\": \"integer\" }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "es.indices.create(index=INDEX_NAME, body=MAPPING_BODY)\n",
    "print(f\"   -> Index '{INDEX_NAME}' created successfully with optimized mapping.\")\n",
    "\n",
    "print(\"\\n3. Starting high-speed Bulk Indexing...\")\n",
    "\n",
    "def generate_actions(df):\n",
    "    \"\"\"pandas DataFrameからElasticsearchのBulk API用アクションを生成\"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        doc = row.to_dict()\n",
    "        yield {\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"_id\": doc[\"PMID\"],\n",
    "            \"_source\": doc\n",
    "        }\n",
    "\n",
    "try:\n",
    "    successes, errors = helpers.bulk(\n",
    "        es, \n",
    "        generate_actions(df),\n",
    "        chunk_size=BULK_SIZE,\n",
    "        request_timeout=60 # タイムアウトを延長\n",
    "    )\n",
    "    print(f\"\\nBulk Indexing Complete.\")\n",
    "    print(f\"   -> Total documents indexed: {len(df)}\")\n",
    "    print(f\"   -> Successfully indexed: {successes}\")\n",
    "    if errors:\n",
    "        print(f\"   -> Errors encountered: {len(errors)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   -> An error occurred during bulk indexing: {e}\")\n",
    "\n",
    "print(\"\\n4. Final Index Count Check:\")\n",
    "try:\n",
    "    count_res = es.count(index=INDEX_NAME)\n",
    "    print(f\"   -> Total documents in ES index '{INDEX_NAME}': {count_res['count']}\")\n",
    "except Exception as e:\n",
    "    print(f\"   -> Error checking document count: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fe307",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_DIR = \"/workspace/0-utils/1-data/pubmed/db/\"\n",
    "\n",
    "es = Elasticsearch(ES_HOST, basic_auth=(\"elastic\", ES_PASSWORD), request_timeout=60)\n",
    "db_files = glob.glob(os.path.join(DB_DIR, \"pubmed_n25_*.db\"))\n",
    "\n",
    "if not db_files:\n",
    "    print(f\"Error: No .db files found in {DB_DIR}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"1. Found {len(db_files)} database files to process.\")\n",
    "\n",
    "MAPPING_BODY = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"english_exact\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"PMID\":        { \"type\": \"keyword\" },\n",
    "      \"TITLE\":       { \"type\": \"text\", \"analyzer\": \"english_exact\" },\n",
    "      \"ABST\":        { \"type\": \"text\", \"analyzer\": \"english_exact\" },\n",
    "      \"ABST_ENG\":    { \"type\": \"integer\" },\n",
    "      \"TRUNCUTATED\": { \"type\": \"integer\" },\n",
    "      \"JOURNAL\":     { \"type\": \"keyword\" },\n",
    "      \"ISSN\":        { \"type\": \"keyword\" },\n",
    "      \"PUB_YEAR\":    { \"type\": \"integer\" },\n",
    "      \"PUB_MONTH\":   { \"type\": \"integer\" }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "print(f\"\\n2. Connecting to Elasticsearch and creating index: {INDEX_NAME}\")\n",
    "try:\n",
    "    if es.indices.exists(index=INDEX_NAME):\n",
    "        print(f\"   -> Index '{INDEX_NAME}' already exists. Deleting...\")\n",
    "        es.indices.delete(index=INDEX_NAME)\n",
    "\n",
    "    es.indices.create(index=INDEX_NAME, body=MAPPING_BODY)\n",
    "    print(f\"   -> Index '{INDEX_NAME}' created successfully with optimized mapping.\")\n",
    "except Exception as e:\n",
    "    print(f\"   -> Error creating index: {e}\")\n",
    "    exit()\n",
    "\n",
    "total_docs_indexed = 0\n",
    "\n",
    "def generate_actions(df_chunk, index_name):\n",
    "    \"\"\"pandas DataFrameからElasticsearchのBulk API用アクションを生成\"\"\"\n",
    "    for index, row in df_chunk.iterrows():\n",
    "        doc = row.to_dict()\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc[\"PMID\"],\n",
    "            \"_source\": doc\n",
    "        }\n",
    "\n",
    "for i, db_file in enumerate(tqdm(db_files)):\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        df = pd.read_sql_query(\"SELECT * FROM raw\", conn)\n",
    "        conn.close()\n",
    "        df = df.fillna(\"\")\n",
    "        \n",
    "        num_records = len(df)\n",
    "        print(f\"   -> Total records in file: {num_records}\")\n",
    "        successes, errors = helpers.bulk(\n",
    "            es, \n",
    "            generate_actions(df, INDEX_NAME),\n",
    "            chunk_size=BULK_SIZE,\n",
    "            request_timeout=120 # タイムアウトをさらに延長\n",
    "        )\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"   -> WARNING: {len(errors)} errors encountered in this batch.\")\n",
    "        \n",
    "        total_docs_indexed += successes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   -> FATAL ERROR processing {os.path.basename(db_file)}: {e}\")\n",
    "\n",
    "print(\"\\n--- Final Summary ---\")\n",
    "print(f\"Total documents successfully processed (sent to ES): {total_docs_indexed}\")\n",
    "\n",
    "try:\n",
    "    count_res = es.count(index=INDEX_NAME)\n",
    "    print(f\"Total unique documents in ES index '{INDEX_NAME}': {count_res['count']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking final document count: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5349aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Searching for 'Coproporphyrin I' in pubmed/ABST ---\n",
      "   -> Total documents found: 196\n",
      "\n",
      "--- Top 3 Results ---\n",
      "   PMID: 1578410\n",
      "   YEAR: 0\n",
      "   TITLE: Porphyrin rings and phospholipids: stimulators of cloning efficiency in certain species of Tetrahymena.\n",
      "------------------------------\n",
      "   PMID: 12830817\n",
      "   YEAR: 0\n",
      "   TITLE: Influence of linker unit on performance of palladium(II) coproporphyrin labelling reagent and its bioconjugates.\n",
      "------------------------------\n",
      "   PMID: 9861496\n",
      "   YEAR: 0\n",
      "   TITLE: Porphyrins in urine, plasma, erythrocytes, bile and faeces in a case of congenital erythropoietic porphyria (Gunther's disease) treated with blood transfusion and iron chelation: lack of benefit from oral charcoal.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "ES_HOST = \"http://elasticsearch:9200\" \n",
    "ES_PASSWORD = \"\" \n",
    "INDEX_NAME = \"pubmed\"\n",
    "BULK_SIZE = 5000\n",
    "\n",
    "query_word = \"Coproporphyrin I\"\n",
    "search_index = \"pubmed\"\n",
    "\n",
    "es = Elasticsearch(\n",
    "    ES_HOST, \n",
    "    basic_auth=(\"elastic\", ES_PASSWORD),\n",
    "    request_timeout=60 \n",
    ")\n",
    "\n",
    "print(f\"\\n--- 5. Searching for '{query_word}' in {search_index}/ABST ---\")\n",
    "\n",
    "search_body = {\n",
    "  \"query\": {\n",
    "    \"multi_match\": {\n",
    "      \"query\": query_word,\n",
    "      \"fields\": [\"ABST\"],\n",
    "      \"type\": \"phrase\"\n",
    "    }\n",
    "  },\n",
    "  \"sort\": [\n",
    "    {\"PUB_YEAR\": {\"order\": \"asc\"}}\n",
    "  ],\n",
    "  \"_source\": [\"PMID\", \"TITLE\", \"PUB_YEAR\"],\n",
    "  \"size\": 3\n",
    "}\n",
    "\n",
    "try:\n",
    "    res = es.search(index=search_index, body=search_body)\n",
    "    \n",
    "    total_hits = res['hits']['total']['value']\n",
    "    \n",
    "    print(f\"   -> Total documents found: {total_hits}\")\n",
    "\n",
    "    if total_hits > 0:\n",
    "        print(\"\\n--- Top 3 Results ---\")\n",
    "        for hit in res['hits']['hits'][:3]:\n",
    "            source = hit['_source']\n",
    "            print(f\"   PMID: {source.get('PMID')}\")\n",
    "            print(f\"   YEAR: {source.get('PUB_YEAR')}\")\n",
    "            print(f\"   TITLE: {source.get('TITLE')}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   -> Error during search query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d101c",
   "metadata": {},
   "source": [
    "# make sentence table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df2daa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_753555/1385969741.py:49: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  successes, errors = helpers.bulk(\n"
     ]
    }
   ],
   "source": [
    "ES_HOST = \"http://elasticsearch:9200\" \n",
    "ES_PASSWORD = \"micgm1Gemini\" \n",
    "DB_PATH = \"/workspace/0-utils/1-data/pubmed/db/pubmed_n25_0000.db\" \n",
    "BULK_SIZE = 5000\n",
    "\n",
    "INDEX_NAME = \"pubmed_sentence\"\n",
    "\n",
    "MAPPING_BODY = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"english_exact\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"PMID\": { \"type\": \"keyword\" }, \n",
    "      \"SENTID\":        { \"type\": \"keyword\" },\n",
    "      \"SENTENCE\":    { \"type\": \"text\", \"analyzer\": \"english_exact\" },\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "def generate_actions(df):\n",
    "    for index, row in df.iterrows():\n",
    "        doc = row.to_dict()\n",
    "        yield {\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"_id\": doc[\"SENTID\"],\n",
    "            \"_source\": doc\n",
    "        }\n",
    "\n",
    "es = Elasticsearch(\n",
    "    ES_HOST, \n",
    "    basic_auth=(\"elastic\", ES_PASSWORD),\n",
    "    request_timeout=60\n",
    ")\n",
    "es.indices.create(index=INDEX_NAME, body=MAPPING_BODY)\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "df = pd.read_sql_query(\"SELECT * FROM sent_split\", conn)\n",
    "conn.close()\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "successes, errors = helpers.bulk(\n",
    "    es, \n",
    "    generate_actions(df),\n",
    "    chunk_size=BULK_SIZE,\n",
    "    request_timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938fb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Connecting to Elasticsearch and creating index: pubmed_sentence\n",
      "   -> Index 'pubmed_sentence' already exists. Deleting...\n",
      "   -> Index 'pubmed_sentence' created successfully with optimized mapping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/382 [00:00<?, ?it/s]/tmp/ipykernel_753555/3716613204.py:34: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  successes, errors = helpers.bulk(\n",
      "  3%|▎         | 13/382 [05:26<2:46:03, 27.00s/it]"
     ]
    }
   ],
   "source": [
    "DB_DIR = \"/workspace/0-utils/1-data/pubmed/db/\"\n",
    "db_files = glob.glob(os.path.join(DB_DIR, \"pubmed_n25_*.db\"))\n",
    "\n",
    "def generate_actions(df, index_name):\n",
    "    for row in df.itertuples(index=False):\n",
    "        doc = row._asdict()\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc.get(\"SENTENCE_ID\"), \n",
    "            \"_source\": doc\n",
    "        }\n",
    "\n",
    "print(f\"\\n2. Connecting to Elasticsearch and creating index: {INDEX_NAME}\")\n",
    "try:\n",
    "    if es.indices.exists(index=INDEX_NAME):\n",
    "        print(f\"   -> Index '{INDEX_NAME}' already exists. Deleting...\")\n",
    "        es.indices.delete(index=INDEX_NAME)\n",
    "\n",
    "    es.indices.create(index=INDEX_NAME, body=MAPPING_BODY)\n",
    "    print(f\"   -> Index '{INDEX_NAME}' created successfully with optimized mapping.\")\n",
    "except Exception as e:\n",
    "    print(f\"   -> Error creating index: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "for i, db_file in enumerate(tqdm(db_files)):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        df = pd.read_sql_query(\"SELECT * FROM sent_split\", conn)\n",
    "        conn.close()\n",
    "        df = df.fillna(\"\")\n",
    "        \n",
    "        num_records = len(df)\n",
    "        successes, errors = helpers.bulk(\n",
    "            es, \n",
    "            generate_actions(df, INDEX_NAME),\n",
    "            chunk_size=BULK_SIZE,\n",
    "            request_timeout=120 # タイムアウトをさらに延長\n",
    "        )\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"   -> WARNING: {len(errors)} errors encountered in this batch.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   -> FATAL ERROR processing {os.path.basename(db_file)}: {e}\")\n",
    "\n",
    "print(\"\\n--- Final Summary ---\")\n",
    "try:\n",
    "    count_res = es.count(index=INDEX_NAME)\n",
    "    print(f\"Total unique documents in ES index '{INDEX_NAME}': {count_res['count']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking final document count: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb3481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
